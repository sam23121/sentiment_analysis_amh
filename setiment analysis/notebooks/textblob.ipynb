{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/lib/python3.7/site-packages/torch/cuda/__init__.py:104: UserWarning: \n",
      "NVIDIA GeForce RTX 3050 Ti Laptop GPU with CUDA capability sm_86 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n",
      "If you want to use the NVIDIA GeForce RTX 3050 Ti Laptop GPU GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3050 Ti Laptop GPU'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/amh_datasetall .txt\")\n",
    "df.head()\n",
    "df2 = pd.read_csv(\"../data/Amharic_dataset.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38424"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [\"texts\", \"labels\"]\n",
    "df2.columns = [\"texts\", \"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([df2, df], ignore_index=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_new[df_new[\"texts\"] == \"ፈጣሪ ይባርካቹ\"])\n",
    "df_new.drop_duplicates(inplace=True)\n",
    "len(df_new[df_new[\"texts\"] == \"ፈጣሪ ይባርካቹ\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ፈጣሪ ይባርካቹ</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       texts    labels\n",
       "0  ፈጣሪ ይባርካቹ  positive"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new[df_new[\"texts\"] == \"ፈጣሪ ይባርካቹ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../data/amahric_testdata.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28677</th>\n",
       "      <td>እረ ቆሻሻውስ ያለ ቤተ መንግስት ውስጥ ያሉትን ካፅዳክ ነው</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28678</th>\n",
       "      <td>በነካካ እጅህ የሀገሬን ዘረኛውን የሀይማኖት ቦታ የሚያቃጠለውን በዛው ብታ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28679</th>\n",
       "      <td>መቸም ሰይጣን ብዙዎችን የጥላቻ መርዝ እያጠጣ ስንቱን ሊገለው ነው ጎበዝ ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28680</th>\n",
       "      <td>ለነገሩ ምን ታድርግ ትላልቅ ስራዎች የመስራትና መምራት አቅም የለህም አሁ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28681</th>\n",
       "      <td>ኣስመሳይ ውሸታም በጭንቅላትህ ያለ ቆሻሻ ብታፀዳ ይሻላል ነበር ድነጋይ ራ...</td>\n",
       "      <td>strongNegative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   texts          labels\n",
       "28677              እረ ቆሻሻውስ ያለ ቤተ መንግስት ውስጥ ያሉትን ካፅዳክ ነው        negative\n",
       "28678  በነካካ እጅህ የሀገሬን ዘረኛውን የሀይማኖት ቦታ የሚያቃጠለውን በዛው ብታ...        positive\n",
       "28679  መቸም ሰይጣን ብዙዎችን የጥላቻ መርዝ እያጠጣ ስንቱን ሊገለው ነው ጎበዝ ...        positive\n",
       "28680  ለነገሩ ምን ታድርግ ትላልቅ ስራዎች የመስራትና መምራት አቅም የለህም አሁ...        negative\n",
       "28681  ኣስመሳይ ውሸታም በጭንቅላትህ ያለ ቆሻሻ ብታፀዳ ይሻላል ነበር ድነጋይ ራ...  strongNegative"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/amahric_testdata.csv\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[\"texts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'neutral', nan], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "texts     በአማራ ክልል በ ደቡብ ወሎ አካባቢ  ከፍተኛ የሆነ ጎርፍ ብዙ ህዝቦችን ...\n",
       "labels                                              neutral\n",
       "Name: 2693, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df[df[\"labels\"] == \"neutrall\"]\n",
    "df.loc[2693]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'neutral', nan], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"labels\"] = df[\"labels\"].str.strip()\n",
    "df[\"labels\"] = df[\"labels\"].str.replace(\"strongPositive\", \"positive\", regex=True)\n",
    "df[\"labels\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [texts, labels]\n",
       "Index: []"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"labels\"] == \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    14154\n",
       "positive    10084\n",
       "neutral      4440\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"labels\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = df[\"labels\"].to_list()\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls = df[\"texts\"].to_list()\n",
    "len(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc_and_special_chars(ls): \n",
    "    new_ls = []\n",
    "    for text in ls:\n",
    "        text = str(text)\n",
    "        normalized_text = re.sub('[\\!\\@\\#\\$\\%\\^\\«\\»\\&\\*\\(\\)\\…\\[\\]\\{\\}\\;\\“\\”\\›\\’\\‘\\\"\\'\\:\\,\\.\\‹\\/\\<\\>\\?\\\\\\\\|\\`\\´\\~\\-\\=\\+\\፡\\።\\፤\\;\\፦\\፥\\፧\\፨\\፠\\፣]', '',text)\n",
    "        new_ls.append(normalized_text)\n",
    "    return new_ls\n",
    "\n",
    "def remove_ascii_and_numbers(ls):\n",
    "    new_ls = []\n",
    "    for text_input in ls:\n",
    "        text_input = str(text_input)\n",
    "        rm_num_and_ascii=re.sub('[A-Za-z0-9]','',text_input)\n",
    "        text = re.sub('[\\'\\u1369-\\u137C\\']+','',rm_num_and_ascii)\n",
    "        new_ls.append(text)\n",
    "    return new_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ls = remove_punc_and_special_chars(ls)\n",
    "new_ls = remove_ascii_and_numbers(new_ls)\n",
    "len(new_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_char_level_missmatch(ls):\n",
    "        new_ls = []\n",
    "        for input_token in ls:\n",
    "                input_token = str(input_token)\n",
    "                rep1=re.sub('[ሃኅኃሐሓኻ]','ሀ',input_token)\n",
    "                rep2=re.sub('[ሑኁዅ]','ሁ',rep1)\n",
    "                rep3=re.sub('[ኂሒኺ]','ሂ',rep2)\n",
    "                rep4=re.sub('[ኌሔዄ]','ሄ',rep3)\n",
    "                rep5=re.sub('[ሕኅ]','ህ',rep4)\n",
    "                rep6=re.sub('[ኆሖኾ]','ሆ',rep5)\n",
    "                rep7=re.sub('[ሠ]','ሰ',rep6)\n",
    "                rep8=re.sub('[ሡ]','ሱ',rep7)\n",
    "                rep9=re.sub('[ሢ]','ሲ',rep8)\n",
    "                rep10=re.sub('[ሣ]','ሳ',rep9)\n",
    "                rep11=re.sub('[ሤ]','ሴ',rep10)\n",
    "                rep12=re.sub('[ሥ]','ስ',rep11)\n",
    "                rep13=re.sub('[ሦ]','ሶ',rep12)\n",
    "                rep14=re.sub('[ዓኣዐ]','አ',rep13)\n",
    "                rep15=re.sub('[ዑ]','ኡ',rep14)\n",
    "                rep16=re.sub('[ዒ]','ኢ',rep15)\n",
    "                rep17=re.sub('[ዔ]','ኤ',rep16)\n",
    "                rep18=re.sub('[ዕ]','እ',rep17)\n",
    "                rep19=re.sub('[ዖ]','ኦ',rep18)\n",
    "                rep20=re.sub('[ጸ]','ፀ',rep19)\n",
    "                rep21=re.sub('[ጹ]','ፁ',rep20)\n",
    "                rep22=re.sub('[ጺ]','ፂ',rep21)\n",
    "                rep23=re.sub('[ጻ]','ፃ',rep22)\n",
    "                rep24=re.sub('[ጼ]','ፄ',rep23)\n",
    "                rep25=re.sub('[ጽ]','ፅ',rep24)\n",
    "                rep26=re.sub('[ጾ]','ፆ',rep25)\n",
    "                #Normalizing words with Labialized Amharic characters such as በልቱዋል or  በልቱአል to  በልቷል  \n",
    "                rep27=re.sub('(ሉ[ዋአ])','ሏ',rep26)\n",
    "                rep28=re.sub('(ሙ[ዋአ])','ሟ',rep27)\n",
    "                rep29=re.sub('(ቱ[ዋአ])','ቷ',rep28)\n",
    "                rep30=re.sub('(ሩ[ዋአ])','ሯ',rep29)\n",
    "                rep31=re.sub('(ሱ[ዋአ])','ሷ',rep30)\n",
    "                rep32=re.sub('(ሹ[ዋአ])','ሿ',rep31)\n",
    "                rep33=re.sub('(ቁ[ዋአ])','ቋ',rep32)\n",
    "                rep34=re.sub('(ቡ[ዋአ])','ቧ',rep33)\n",
    "                rep35=re.sub('(ቹ[ዋአ])','ቿ',rep34)\n",
    "                rep36=re.sub('(ሁ[ዋአ])','ኋ',rep35)\n",
    "                rep37=re.sub('(ኑ[ዋአ])','ኗ',rep36)\n",
    "                rep38=re.sub('(ኙ[ዋአ])','ኟ',rep37)\n",
    "                rep39=re.sub('(ኩ[ዋአ])','ኳ',rep38)\n",
    "                rep40=re.sub('(ዙ[ዋአ])','ዟ',rep39)\n",
    "                rep41=re.sub('(ጉ[ዋአ])','ጓ',rep40)\n",
    "                rep42=re.sub('(ደ[ዋአ])','ዷ',rep41)\n",
    "                rep43=re.sub('(ጡ[ዋአ])','ጧ',rep42)\n",
    "                rep44=re.sub('(ጩ[ዋአ])','ጯ',rep43)\n",
    "                rep45=re.sub('(ጹ[ዋአ])','ጿ',rep44)\n",
    "                rep46=re.sub('(ፉ[ዋአ])','ፏ',rep45)\n",
    "                rep47=re.sub('[ቊ]','ቁ',rep46) #ቁ can be written as ቊ\n",
    "                rep48=re.sub('[ኵ]','ኩ',rep47) #ኩ can be also written as ኵ  \n",
    "                new_ls.append(rep48)\n",
    "        return new_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ls = normalize_char_level_missmatch(new_ls)\n",
    "len(new_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ls2 = []\n",
    "for text in new_ls:\n",
    "    text = re.sub(\"[\\ufeff]\",'',text)\n",
    "    new_ls2.append(text)\n",
    "len(new_ls2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-aa464e9d90d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_text2 = ' '.join(new_ls2)\n",
    "# create a list of words\n",
    "words = all_text2.split()\n",
    "# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374434"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28682\n"
     ]
    }
   ],
   "source": [
    "reviews_int = []\n",
    "for review in new_ls2:\n",
    "    r = [vocab_to_int[w] for w in review.split()]\n",
    "    reviews_int.append(r)\n",
    "# print (reviews_int[0:3])\n",
    "print(len(reviews_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_labels = [1 if label =='positive' elif label == \"\" 0 for label in labels_split]\n",
    "import numpy as np\n",
    "encoded_labels = []\n",
    "for label in labels:\n",
    "    # if label == \"strongNegative\":\n",
    "    #     encoded_labels.append(0)\n",
    "    if label == \"negative\":\n",
    "        encoded_labels.append(0)\n",
    "    # elif label == \"neutral\":\n",
    "    #     encoded_labels.append(1)\n",
    "    else:\n",
    "        encoded_labels.append(1)\n",
    "    # else:\n",
    "    #     encoded_labels.append(4)\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "count    28682.000000\n",
       "mean        13.054668\n",
       "std         17.990737\n",
       "min          0.000000\n",
       "25%          6.000000\n",
       "50%          9.000000\n",
       "75%         15.000000\n",
       "max        758.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "reviews_len = [len(x) for x in reviews_int]\n",
    "pd.Series(reviews_len).hist()\n",
    "plt.show()\n",
    "pd.Series(reviews_len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28682"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l>0 ]\n",
    "encoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l> 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_int, seq_length):\n",
    "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    '''\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0    64 18125]\n",
      " [    0     0     0     0     0     0     0     0     0     0  3342  2266\n",
      "  32525 32526     8    33    87  4342   810  4343]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0    44     1     4 18126  3343]\n",
      " [    0     0     0     0     0     0   322  1798  6901     8 32527   375\n",
      "  32528  1870    25 12792   572  8153  9902  1870]\n",
      " [    0     0   331 18127  2267   138 18128   286  6902  6903 18129 18130\n",
      "   4793     8 18131 18132  1594 18133  1417   159]\n",
      " [32529 32530 32531  3624 32532   136    45  1949 32533    17  6904 32534\n",
      "   1418 32535 32536     1   290   513   663   109]\n",
      " [    0 12793  1082   216 18134   573  3344  6018  2924 12794   323 32537\n",
      "     22 32538  3125 32539 32540  5333     5   891]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "    541  2736  8154  5334     1   332  1083     6]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0 18135  1022 18136     1 12795 18136     1]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0   113    74   862   278 12796 12797    74]]\n"
     ]
    }
   ],
   "source": [
    "features = pad_features(reviews_int, 20)\n",
    "print (features[:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_frac = 0.8\n",
    "train_x = features[0:int(split_frac*len(features))]\n",
    "train_y = encoded_labels[0:int(split_frac*len(features))]\n",
    "remaining_x = features[int(split_frac*len(features)):]\n",
    "remaining_y = encoded_labels[int(split_frac*len(features)):]\n",
    "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
    "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
    "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
    "test_y = remaining_y[int(len(remaining_y)*0.5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,     0,    64, 18125],\n",
       "       [    0,     0,     0, ...,  4342,   810,  4343],\n",
       "       [    0,     0,     0, ...,     4, 18126,  3343],\n",
       "       ...,\n",
       "       [    0,     0,     0, ..., 12997, 20708,  2844],\n",
       "       [    0,     0,     0, ..., 16939,    38, 80926],\n",
       "       [    0,     0,     0, ..., 22862,  5298,    61]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(np.array(train_x)), torch.from_numpy(np.array(train_y)))\n",
    "valid_data = TensorDataset(torch.from_numpy(np.array(valid_x)), torch.from_numpy(np.array(valid_y)))\n",
    "test_data = TensorDataset(torch.from_numpy(np.array(test_x)), torch.from_numpy(np.array(test_y)))\n",
    "# dataloaders\n",
    "batch_size = 32\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([32, 20])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,     0,     0,     0,   519,     3,   572,  1281,\n",
      "         56578,   187,  6633,   544,    32,   101,    21, 56579,    23, 56580],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "         53161,  2727,  3210,     1,    72, 16772, 53162,   120,  2165, 53163],\n",
      "        [    0,     0, 26928,   799,  1530,   676,  6876,   904,     9, 17220,\n",
      "         12244,    10,    83,   906, 30672,   285, 30673,   800,   490,   239],\n",
      "        [ 2744,  1236,   310,   241,   441, 23781, 23782, 23783,   361, 23784,\n",
      "          1627,  6303,   867, 23785,  6336,  3836,    17,  7393,  4861,   481],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0, 26581, 26582,   247,    52,    71, 49511],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,  1455,  2381,    42,\n",
      "         10901,  4767,   656,   698,     1, 31436,  4579, 12614,    58,  9614],\n",
      "        [    0,  1697, 72009,  1512, 72010,   469,    61,     2, 72011, 18236,\n",
      "          5990, 26953,     2, 17759,   760,   999, 72012,     2, 72013,    10],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "          7642,    24,  1539,   370, 50217,   152,     7, 24687,  4272,   114],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,   640, 76897,  1010],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0, 51861, 51862,     1],\n",
      "        [    0,     0,     0,     0,     0, 18008,   130,  9530, 16223,   102,\n",
      "          9444,   553, 78377, 78378,  3323,  1803, 10625,   194, 31795, 31562],\n",
      "        [ 3094, 11928, 47182,   924,     2, 47183,    59,   307, 47184,  3306,\n",
      "           993,  7306,   431,     2,   564,  9969,    60, 47185, 47186, 47187],\n",
      "        [   66,  1854,    14, 16032,  1590,    22,   173, 16033,   414, 11821,\n",
      "            33,   967, 16034,  9275,   339,  1588,     6,     3,  4293, 16035],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,   590,   313,    16,    18,   196],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0, 57505,\n",
      "         14875, 57506, 57507, 57508,   190, 10235,   107, 57509,   347,   894],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,  4434, 26462,   417,  1982,  3491, 48909],\n",
      "        [    0,     0,     0,     0,    92,   295, 18000, 17105, 78478, 78479,\n",
      "          4578, 78480,  5964,    11,  1077, 11647,    76,    20,     8, 78481],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0, 67932,   144, 14529, 67933,     1,  9355],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,   579,\n",
      "             1,   168,  1274,  1618,  5542,  1274, 10060, 57030, 57031, 57032],\n",
      "        [    0,     0,     0, 63178, 63179,     3,     5,  3812,  7734,   493,\n",
      "             2, 28378, 63180, 63181,   626,  2648, 63182,    43, 63183,  3921],\n",
      "        [    0,     0,     0,     0,    47,    32,    75, 59405,   301, 59406,\n",
      "         59407,  1584, 28478,  9794,     1,   118,   535,     3, 59408,  2846],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0, 75359,  8539,  4779, 75360, 31562],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,    31,   177,   248],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,  3256,   565,    67,  3101,  1260,  1313,    95, 15319,  5238],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "          1379,   470,  6801,  4189,  8070,  6857, 17490,    59,     1, 31955],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,   597,   954,   651,  1253,   472,    49,     8,  6866,   202],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0, 49959,\n",
      "         49960, 16495, 49961,  5461,    26,  1050,   324,   432,    37,    95],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,   316,   247,  1353,   400],\n",
      "        [  351,  1607, 10160,    21,   486,   714,    71,    50,    37,    21,\n",
      "           486,   994,    71,  2319,  2776, 10161, 10162,   126,  1189,    15],\n",
      "        [10953,  3073,    53,  1294,    12, 10096,  2962,  7614, 14769,  1351,\n",
      "         14770,  1477,   173,   266,    43, 14771,   302,  4462,   278,  4210],\n",
      "        [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,  5954,    17,    62,   834,     4],\n",
      "        [12238,   514,    18,   636,   514,  2428,   116, 18053,    45,  5298,\n",
      "         10838,   514,    18,   953,   513,    91,    74,    70,    60,   297]])\n",
      "\n",
      "Sample label size:  torch.Size([32])\n",
      "Sample label: \n",
      " tensor([0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
      "        1, 1, 0, 1, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "train_on_gpu = False\n",
    "class SentimentLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid() # for outputs more than 1 Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                        weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentLSTM(\n",
      "  (embedding): Embedding(90858, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1 # 3 for our case\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "net = SentimentLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 100... Loss: 0.062056... Val Loss: 1.785001\n",
      "Epoch: 1/3... Step: 200... Loss: 0.004482... Val Loss: 1.639351\n",
      "Epoch: 1/3... Step: 300... Loss: 0.004832... Val Loss: 1.598240\n",
      "Epoch: 1/3... Step: 400... Loss: 0.017610... Val Loss: 1.469717\n",
      "Epoch: 1/3... Step: 500... Loss: 0.006515... Val Loss: 1.287389\n",
      "Epoch: 1/3... Step: 600... Loss: 0.043338... Val Loss: 1.269049\n",
      "Epoch: 1/3... Step: 700... Loss: 0.002272... Val Loss: 1.403488\n",
      "Epoch: 2/3... Step: 800... Loss: 0.014628... Val Loss: 1.506045\n",
      "Epoch: 2/3... Step: 900... Loss: 0.000814... Val Loss: 1.574456\n",
      "Epoch: 2/3... Step: 1000... Loss: 0.049310... Val Loss: 1.591256\n",
      "Epoch: 2/3... Step: 1100... Loss: 0.011937... Val Loss: 1.479115\n",
      "Epoch: 2/3... Step: 1200... Loss: 0.000275... Val Loss: 1.655437\n",
      "Epoch: 2/3... Step: 1300... Loss: 0.005120... Val Loss: 1.347513\n",
      "Epoch: 2/3... Step: 1400... Loss: 0.054998... Val Loss: 1.472010\n",
      "Epoch: 3/3... Step: 1500... Loss: 0.004633... Val Loss: 1.669067\n",
      "Epoch: 3/3... Step: 1600... Loss: 0.060974... Val Loss: 1.548797\n",
      "Epoch: 3/3... Step: 1700... Loss: 0.001535... Val Loss: 1.760927\n",
      "Epoch: 3/3... Step: 1800... Loss: 0.065518... Val Loss: 1.652355\n",
      "Epoch: 3/3... Step: 1900... Loss: 0.001979... Val Loss: 1.862780\n",
      "Epoch: 3/3... Step: 2000... Loss: 0.018405... Val Loss: 1.605837\n",
      "Epoch: 3/3... Step: 2100... Loss: 0.005250... Val Loss: 1.475450\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# loss and optimization functions\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss() # for more than one output CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "# training params\n",
    "\n",
    "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "# if(train_on_gpu):\n",
    "#     net.cuda()\n",
    "\n",
    "net.train()\n",
    "train_on_gpu=False\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if train_on_gpu:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                # if(train_on_gpu):\n",
    "                #     inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                inputs = inputs.type(torch.LongTensor)\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.779\n",
      "Test accuracy: 0.661\n"
     ]
    }
   ],
   "source": [
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # if(train_on_gpu):\n",
    "    #     inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    inputs = inputs.type(torch.LongTensor)\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) \n",
    "    np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fed9591a510>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2312, 1, 47, 1855, 18, 1852]]\n",
      "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0 2312    1\n",
      "    47 1855   18 1852]]\n",
      "torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from string import punctuation\n",
    "\n",
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower() # lowercase\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
    "\n",
    "    return test_ints\n",
    "\n",
    "# test code and generate tokenized review\n",
    "test_ints = tokenize_review(\"ሙከራ ነው ይሄ ቢሰራ ደስ ይለኛል\")\n",
    "print(test_ints)\n",
    "\n",
    "\n",
    "# test sequence padding\n",
    "seq_length=200\n",
    "features = pad_features(test_ints, seq_length)\n",
    "\n",
    "print(features)\n",
    "\n",
    "\n",
    "# test conversion to tensor and pass into your model\n",
    "feature_tensor = torch.from_numpy(features)\n",
    "print(feature_tensor.size())\n",
    "\n",
    "\n",
    "def predict(net, test_review, sequence_length=200):\n",
    "    \n",
    "    net.eval()\n",
    "    \n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features(test_ints, seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = torch.from_numpy(features)\n",
    "    \n",
    "    batch_size = feature_tensor.size(0)\n",
    "    \n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        feature_tensor = feature_tensor.cuda()\n",
    "    \n",
    "    # get the output from the model\n",
    "    output, h = net(feature_tensor, h)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze()) \n",
    "    # printing output value, before rounding\n",
    "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
    "    \n",
    "    # print custom response\n",
    "    if(pred.item()==1):\n",
    "        print(\"Positive review detected!\")\n",
    "    else:\n",
    "        print(\"Negative review detected.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.000029\n",
      "Negative review detected.\n"
     ]
    }
   ],
   "source": [
    "test_review = 'እነዝ አማራ መጥፋት አለባቸው'\n",
    "seq_length=200 \n",
    "predict(net, test_review, seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value, pre-rounding: 0.999973\n",
      "Positive review detected!\n"
     ]
    }
   ],
   "source": [
    "test_review = 'ሙከራ ነው ይሄ ቢሰራ ደስ ይለኛል'\n",
    "seq_length=200 \n",
    "predict(net, test_review, seq_length)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "matrix = CountVectorizer(analyzer='word',max_features=1000,ngram_range=(1, 3))\n",
    "X = matrix.fit_transform(new_ls2).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_label = list(set(labels))\n",
    "Y= []\n",
    "for i in labels:\n",
    "    Y.append(unique_label.index(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3871361338678752"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict Class\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b15b47e994a45b2c1c9c2019b348cb81bbb68780db9285c808c045d049d064a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
